{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafío - Inferencia de tópicos con EM\n",
    "\n",
    "**Nombre alumno:** Julio Valdés\n",
    "\n",
    "Profesor: Gabriel Tamayo L.\n",
    "\n",
    "Generación: G5\n",
    "\n",
    "* Para realizar este desafío debes haber revisado la lectura y videos correspondiente a la unidad.\n",
    "* Crea una carpeta de trabajo y guarda todos los archivos correspondientes (notebook y csv).\n",
    "* Una vez terminado el desafío, comprime la carpeta y sube el .zip a la sección correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "* En esta sesión trabajaremos con una serie de base de datos sobre letras musicales de distintos artistas. Cada uno de los csv se encuentra en la carpeta dump .\n",
    "* Cada csv tiene el nombre del artista a analizar. Los archivos contienen el nombre del artista, el género musical del artista, el nombre de la canción y las letras.\n",
    "* En base a esta información, el objetivo del ejercicio es generar un modelo probabilístico que pueda identificar el género musical más probable dado la letra de una canción.\n",
    "* Para ello implementaremos un modelo conocido como Latent Dirichlet Allocation que hace uso de una variante del algoritmo EM para inferir clases latentes a partir de una matriz de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Preparar el ambiente de trabajo\n",
    "* Importe los módulos numpy, pandas, matplotlib, seaborn, glob y os siguiendo las buenas prácticas. Los últimos dos módulos permitirán realizar la importación de múltiples archivos dentro de la carpeta dump.\n",
    "* Para ello genere un objeto que guarde en una lista todos los archivos alojados en dump utilizando y para extraer las rutas absolutas. Posteriormente genere un objeto que contenga todos los csv.\n",
    "* Asegúrese de eliminar la columna Unnamed: 0 que se genera por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Matriz de ocurrencias\n",
    "* Importe la clase dentro de los módulos feature_extraction.text de la librería .\n",
    "* Aplique la clase para extraer las 5000 palabras más repetidas en toda la base de datos.\n",
    "* Con la clase inicializada, incorpore las letras con el método fit_transform y guarde los resultados en un nuevo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Entrenamiento del Modelo\n",
    "* Importe `sklearn.decomposition.LatentDirichletAllocation` y `sklearn.model_selection.GridSearchCV`\n",
    "* Genere una búsqueda de grilla con los siguientes hiperparámetros:\n",
    "  * `n_components: [5, 10, 15]`\n",
    "  * `learning_decay: [0.7, 0.5]`\n",
    "* Entrene la búsqueda de grilla con las letras en un formato vectorizado con `CountVectorizer`\n",
    "* Reporte brevemente cuál es la mejor combinación de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4 : Inferencia e Identificación de Tópicos\n",
    "* En base a la mejor combinación de hiperparámetros, entrene el modelo con la matriz de atributos de las letras.\n",
    "* Para identificar de qué se trata cada tópico, necesitamos identificar las principales 15 palabras asociadas con éste. Puede implementar la siguiente línea de código para identificar las principales palabras en un tópico:\n",
    "```python\n",
    "# mediante .components_ podemos extraer una matriz que entrega las distribución de palabras por cada tópico.\n",
    "for topic_id, topic_name in enumerate(fit_best_lda.components_):\n",
    "# para cada tópico\n",
    "print(\"tópico: {}\".format(topic_id + 1))\n",
    "# mediante argsort logramos ordenar los elementos por magnitud\n",
    "# para los elementos más relevantes ordenados por argsort, buscamos su correlativo\n",
    "# en la matriz dispersa y devolvemos el nombre.\n",
    "# finalmente concatenamos las palabras\n",
    "print(\" \".join([counter.get_feature_names()[i] for i in topic_name.argsort()[:-15 - 1: -1]]))\n",
    "```\n",
    "* Comente a qué tópicos está asociada cada clase inferida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Identificación de probabilidades\n",
    "* En base a la información generada, es posible identificar cuales van a ser los géneros más probables de ocurrir para un artista.\n",
    "* Para ello necesitamos guardar la probabilidad de cada canción en nuestra base de datos original. Podemos implementar esto de la siguiente manera:\n",
    "```python\n",
    "# generamos una transformación de los datos a distribución de tópico por palabra en el documento\n",
    "fit_best_lda = best_lda.transform(transformed_feats)\n",
    "# estra transformación la podemos coercionar a un dataframe de la siguiente manera\n",
    "topics_for_each_doc = pd.DataFrame(\n",
    "    # pasamos esta matriz y la redondeamos en 3 decimales\n",
    "    np.round(fit_best_lda, 3),\n",
    "    # agregamos un índice\n",
    "    index=df_lyrics.index\n",
    ")\n",
    "#agregamos identificadores de columna\n",
    "topics_for_each_doc.columns = list(map(lambda x: \"T: {}\".format(x), range(1, best_lda.n_components + 1)))\n",
    "# concatenamos las probabilidades de tópico por documento a nuestra matriz original\n",
    "concatenated_df = pd.concat([df_lyrics, topics_for_each_doc], axis=1)\n",
    "# argmax en la matriz de tópicos\n",
    "concatenated_df['highest_topic'] = np.argmax(docs_topics.values, axis=1) + 1\n",
    "```\n",
    "* Genere una matriz de correlaciones entre la probabilidad de tópicos inferidos. Comente brevemente cuales son las principales asociaciones existentes.\n",
    "* Con esta nueva base de datos, identifique las probabilidades de pertenencia para un artista específico.\n",
    "* Grafique la distribución de las probabilidades para algún artista en específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
